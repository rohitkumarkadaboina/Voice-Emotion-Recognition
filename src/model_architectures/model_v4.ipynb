{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a236e1b3",
   "metadata": {},
   "source": [
    "# Improve the Model (Version 4)\n",
    " 🔧 What We’ll Do:\n",
    "- Make the model deeper: More Conv1D + BatchNorm + Dropout\n",
    "- Use Bidirectional LSTM to better capture emotion flow\n",
    "- Stratified split so all emotion classes are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5be380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694733c0",
   "metadata": {},
   "source": [
    "Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d126cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1902, 59, 173), y shape: (1902,)\n"
     ]
    }
   ],
   "source": [
    "X = np.load(\"../../data/features/features_v4.npy\") #data\\features\\features_v4.npy\n",
    "y = np.load(\"../../data/features/labels_v4.npy\")\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2484db67",
   "metadata": {},
   "source": [
    "Encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6988405",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d39ac",
   "metadata": {},
   "source": [
    "Train-test split with stratify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3fdc834",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_categorical, test_size=0.2, random_state=42, stratify=y_categorical\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c96d4a9",
   "metadata": {},
   "source": [
    "Build better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de19c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the improved model\n",
    "model = Sequential([\n",
    "    Input(shape=(59, 173)),                              # ✅ Corrected input\n",
    "    Conv1D(64, 5, padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(2),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Conv1D(128, 5, padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(y_categorical.shape[1], activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15aba2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbf3f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: Callbacks\n",
    "# callbacks = [\n",
    "#     EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "#     ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0373da92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.5550 - loss: 1.1849 - val_accuracy: 0.4672 - val_loss: 1.4415\n",
      "Epoch 2/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.5845 - loss: 1.1173 - val_accuracy: 0.4672 - val_loss: 1.4334\n",
      "Epoch 3/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5621 - loss: 1.1814 - val_accuracy: 0.4751 - val_loss: 1.4309\n",
      "Epoch 4/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.5760 - loss: 1.1575 - val_accuracy: 0.4751 - val_loss: 1.4335\n",
      "Epoch 5/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5570 - loss: 1.1449 - val_accuracy: 0.4777 - val_loss: 1.4347\n",
      "Epoch 6/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5621 - loss: 1.1743 - val_accuracy: 0.4829 - val_loss: 1.4455\n",
      "Epoch 7/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.5812 - loss: 1.1396 - val_accuracy: 0.4803 - val_loss: 1.4344\n",
      "Epoch 8/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - accuracy: 0.5538 - loss: 1.1618 - val_accuracy: 0.4829 - val_loss: 1.4457\n",
      "Epoch 9/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 77ms/step - accuracy: 0.5885 - loss: 1.1047 - val_accuracy: 0.4856 - val_loss: 1.4441\n",
      "Epoch 10/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.5821 - loss: 1.0907 - val_accuracy: 0.4829 - val_loss: 1.4422\n",
      "Epoch 11/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.5999 - loss: 1.0631 - val_accuracy: 0.4829 - val_loss: 1.4456\n",
      "Epoch 12/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.5898 - loss: 1.1226 - val_accuracy: 0.4829 - val_loss: 1.4486\n",
      "Epoch 13/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5684 - loss: 1.1327 - val_accuracy: 0.4803 - val_loss: 1.4642\n",
      "Epoch 14/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.6086 - loss: 1.0698 - val_accuracy: 0.4803 - val_loss: 1.4610\n",
      "Epoch 15/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.5687 - loss: 1.1368 - val_accuracy: 0.4803 - val_loss: 1.4457\n",
      "Epoch 16/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.5553 - loss: 1.1362 - val_accuracy: 0.4829 - val_loss: 1.4453\n",
      "Epoch 17/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5874 - loss: 1.1305 - val_accuracy: 0.4856 - val_loss: 1.4465\n",
      "Epoch 18/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5864 - loss: 1.1263 - val_accuracy: 0.4908 - val_loss: 1.4486\n",
      "Epoch 19/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.5806 - loss: 1.0966 - val_accuracy: 0.4803 - val_loss: 1.4480\n",
      "Epoch 20/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5739 - loss: 1.0977 - val_accuracy: 0.4829 - val_loss: 1.4513\n",
      "Epoch 21/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5678 - loss: 1.1369 - val_accuracy: 0.4829 - val_loss: 1.4505\n",
      "Epoch 22/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5886 - loss: 1.1025 - val_accuracy: 0.4856 - val_loss: 1.4561\n",
      "Epoch 23/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.5837 - loss: 1.1258 - val_accuracy: 0.4856 - val_loss: 1.4545\n",
      "Epoch 24/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5772 - loss: 1.1105 - val_accuracy: 0.4777 - val_loss: 1.4592\n",
      "Epoch 25/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.5971 - loss: 1.1210 - val_accuracy: 0.4882 - val_loss: 1.4510\n",
      "Epoch 26/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.5831 - loss: 1.1009 - val_accuracy: 0.4829 - val_loss: 1.4611\n",
      "Epoch 27/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5613 - loss: 1.1414 - val_accuracy: 0.4777 - val_loss: 1.4607\n",
      "Epoch 28/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6048 - loss: 1.1111 - val_accuracy: 0.4908 - val_loss: 1.4393\n",
      "Epoch 29/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5961 - loss: 1.0992 - val_accuracy: 0.4829 - val_loss: 1.4463\n",
      "Epoch 30/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.6037 - loss: 1.0759 - val_accuracy: 0.4908 - val_loss: 1.4414\n",
      "Epoch 31/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5870 - loss: 1.1235 - val_accuracy: 0.4829 - val_loss: 1.4515\n",
      "Epoch 32/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5853 - loss: 1.0975 - val_accuracy: 0.4882 - val_loss: 1.4529\n",
      "Epoch 33/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.5790 - loss: 1.1066 - val_accuracy: 0.4908 - val_loss: 1.4538\n",
      "Epoch 34/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.6060 - loss: 1.0903 - val_accuracy: 0.4934 - val_loss: 1.4505\n",
      "Epoch 35/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.5911 - loss: 1.0949 - val_accuracy: 0.4908 - val_loss: 1.4598\n",
      "Epoch 36/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5947 - loss: 1.0946 - val_accuracy: 0.4908 - val_loss: 1.4544\n",
      "Epoch 37/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.6165 - loss: 1.0538 - val_accuracy: 0.4829 - val_loss: 1.4625\n",
      "Epoch 38/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5816 - loss: 1.1207 - val_accuracy: 0.4856 - val_loss: 1.4533\n",
      "Epoch 39/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.5923 - loss: 1.1054 - val_accuracy: 0.4856 - val_loss: 1.4640\n",
      "Epoch 40/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5961 - loss: 1.0985 - val_accuracy: 0.4856 - val_loss: 1.4652\n",
      "Epoch 41/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.5966 - loss: 1.0752 - val_accuracy: 0.4829 - val_loss: 1.4714\n",
      "Epoch 42/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.6061 - loss: 1.0672 - val_accuracy: 0.4882 - val_loss: 1.4550\n",
      "Epoch 43/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.5770 - loss: 1.1200 - val_accuracy: 0.4908 - val_loss: 1.4553\n",
      "Epoch 44/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5807 - loss: 1.1251 - val_accuracy: 0.4908 - val_loss: 1.4611\n",
      "Epoch 45/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.6025 - loss: 1.0802 - val_accuracy: 0.4908 - val_loss: 1.4489\n",
      "Epoch 46/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5936 - loss: 1.0492 - val_accuracy: 0.4882 - val_loss: 1.4530\n",
      "Epoch 47/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.6023 - loss: 1.0658 - val_accuracy: 0.4882 - val_loss: 1.4526\n",
      "Epoch 48/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5767 - loss: 1.1154 - val_accuracy: 0.4856 - val_loss: 1.4476\n",
      "Epoch 49/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.6125 - loss: 1.0586 - val_accuracy: 0.4777 - val_loss: 1.4616\n",
      "Epoch 50/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5882 - loss: 1.0729 - val_accuracy: 0.4803 - val_loss: 1.4701\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    # callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a95d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"emotion_model_v4.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2236c75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5054 - loss: 1.4017\n",
      "Test accuracy (v4): 0.4803149700164795\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy (v4):\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b30e458",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a455761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib  # assuming you saved your LabelEncoder\n",
    "import os\n",
    "\n",
    "from src.preprocessing.preprocess import extract_features_from_audio  \n",
    "# ------------------ Step 1: Load Pretrained Model ------------------\n",
    "model = tf.keras.models.load_model(\"emotion_model_v4.keras\")\n",
    "\n",
    "# ------------------ Step 2: Load Label Encoder ------------------\n",
    "# Either load it if saved, or recreate and fit it again if not saved\n",
    "# Option A: Load saved encoder\n",
    "# encoder = joblib.load(\"label_encoder.pkl\")\n",
    "\n",
    "# Option B: Recreate and fit using original labels\n",
    "original_labels = np.load(\"labels_v4.npy\")\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(original_labels)\n",
    "\n",
    "# ------------------ Step 3: Feature Extraction Function ------------------\n",
    "def extract_features(file_path, max_pad_len=173):\n",
    "    y, sr = librosa.load(file_path, sr=22050)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    \n",
    "    # Padding or truncating to fixed length\n",
    "    if mfccs.shape[1] < max_pad_len:\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfccs = mfccs[:, :max_pad_len]\n",
    "    \n",
    "    return mfccs\n",
    "\n",
    "# ------------------ Step 4: Load and Preprocess New Sample ------------------\n",
    "file_path = \"sample.wav\"  # replace with your new file\n",
    "mfcc_features = extract_features_from_audio(file_path)  # shape: (40, 173)\n",
    "mfcc_features = np.expand_dims(mfcc_features, axis=0)  # shape: (1, 40, 173)\n",
    "\n",
    "# ------------------ Step 5: Predict ------------------\n",
    "prediction = model.predict(mfcc_features)\n",
    "predicted_label_index = np.argmax(prediction)\n",
    "predicted_emotion = encoder.inverse_transform([predicted_label_index])[0]\n",
    "\n",
    "print(\"Predicted Emotion:\", predicted_emotion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
