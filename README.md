# ğŸ™ï¸ Voice Emotion Recognition using Deep Learning

This project focuses on classifying human emotions from speech using deep learning techniques. Using the **RAVDESS** dataset, we explore advanced audio preprocessing, feature engineering, and neural network architectures to accurately detect emotions from voice signals.

---

## ğŸ” Project Overview

- Built a deep learning pipeline to classify 8 emotional states: **Angry, Calm, Disgust, Fearful, Happy, Neutral, Sad, Surprised**
- Used **log-Mel spectrograms** and **derivative features (delta, deltaÂ²)** for rich temporal-spectral representation.
- Applied **data augmentation** strategies (noise addition, pitch shifting, time-stretching) to improve generalization.
- Trained and evaluated multiple models with **Conv1D + BiGRU** based architectures.

---

## ğŸ“ Folder Structure

```

voice\_emotion\_recognition/
â”‚
â”œâ”€â”€ data/                  # All dataset-related files
â”‚   â””â”€â”€ features/
â”‚       â”œâ”€â”€ features.npy               # Processed features
â”‚       â”œâ”€â”€ labels.npy                 # Corresponding labels
â”‚       â”œâ”€â”€ features\_v4.npy            # Alt version (e.g., different pipeline)
â”‚       â””â”€â”€ labels\_v4.npy
â”‚   â””â”€â”€ raw/ravdess/                  # Raw RAVDESS audio data
â”‚       â”œâ”€â”€ Actor\_01/
â”‚       â”œâ”€â”€ Actor\_02/
â”‚       â””â”€â”€ ... Actor\_24/
â”‚
â”œâ”€â”€ models/                # Saved trained models
â”‚   â”œâ”€â”€ emotional\_model.h5
â”‚   â”œâ”€â”€ emotional\_model\_v2.keras
â”‚   â”œâ”€â”€ emotional\_model\_v3.keras
â”‚   â””â”€â”€ emotional\_model\_v4.keras
â”‚
â”œâ”€â”€ src/                   # Main scripts
â”‚   â”œâ”€â”€ model\_architectures/
â”‚   â”‚   â”œâ”€â”€ model.ipynb
â”‚   â”‚   â”œâ”€â”€ model\_v2.ipynb
â”‚   â”‚   â””â”€â”€ model\_v4.ipynb
â”‚   â””â”€â”€ preprocessing/
â”‚       â””â”€â”€ preprocess.py
â”‚
â”œâ”€â”€ utils/                 # Utility functions
â”‚   â””â”€â”€ augment.py         # Audio augmentation helpers
â”‚
â””â”€â”€ requirements.txt       # Python dependencies

````

---

## ğŸš€ Features & Highlights

- âœ… **Advanced Spectral Features**: Extracted log-Mel spectrograms along with temporal derivatives (Î”, Î”Â²) to enhance emotional cues in audio.
- âœ… **Robust Augmentation**: Applied real-world inspired augmentations â€” noise, pitch shift, and time-stretch â€” to create a resilient model.
- âœ… **Temporal Modeling**: Used time-aware Conv1D layers followed by BiGRU for effective emotion detection from voice sequences.
- âœ… **Modular Codebase**: Clean separation of preprocessing, model building, and utilities for easy experimentation and extension.

---

## ğŸ“Š Model Evaluation

Performance evaluated using standard metrics:

- **Precision, Recall, F1-Score**
- **Class-wise Emotion Analysis**
- **Confusion Matrix Visualization**

> Achieved **~59% accuracy** on an 8-class classification task, with strong performance on *Angry*, *Disgust*, *Calm*, and *Surprised* emotions.

---

## ğŸ“¦ Requirements

Install the required dependencies with:

```bash
pip install -r requirements.txt
````

---

## ğŸ—ƒï¸ Dataset

* [RAVDESS Dataset](https://zenodo.org/record/1188976)

  * 24 professional actors vocalizing two lexically-matched statements in a neutral North American accent
  * Emotions: calm, happy, sad, angry, fearful, surprise, disgust, neutral

---

## ğŸ§  Future Enhancements

* Integrate attention layers for improved temporal focus.
* Try transformer-based models for long-sequence encoding.
* Extend to multilingual or real-time emotion recognition.

---

## ğŸ“Œ Credits

* Audio Data: [RAVDESS - Livingstone & Russo, 2018](https://zenodo.org/record/1188976)
* Libraries used: `Librosa`, `TensorFlow/Keras`, `NumPy`, `Matplotlib`

---

## ğŸ“œ License

This project is for educational purposes. Please ensure RAVDESS terms are followed for any public sharing.

---

## ğŸ¤ Contributions

Feel free to fork the repo, suggest improvements, or raise issues!
